experiment_type: nn
Architecture:
  class: Transformer
  parameters:
    seq_len: 24
    d_model: 256
    n_heads: 8
    num_layers: 3
    est_head_dim: 256
    output_size: 1
    dropout_rate: 0.02328683945126167
    est_head_dropout: 0.05842565836667351
    activation: "relu"
Training:
  seed: 42
  dataset_id: BeijingPM25Quality
  dataset_class: SequentialDataset
  batch_size: 256
  num_epochs: 100
  optimizer: Adam
  optimizer_params:
    lr: 0.00020954238527899396
  loss_function: MSELoss
