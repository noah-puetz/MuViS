experiment_type: nn
Architecture:
  class: Transformer
  parameters:
    seq_len: 120
    d_model: 256
    n_heads: 16
    num_layers: 4
    est_head_dim: 256
    output_size: 1
    dropout_rate: 0.1956939558027962
    est_head_dropout: 0.0034904328608135887
    activation: "relu"
Training:
  seed: 42
  dataset_id: Panasonic18650PFData
  dataset_class: SequentialDataset
  batch_size: 256
  num_epochs: 100
  optimizer: Adam
  optimizer_params:
    lr: 0.0002247203847109756
  loss_function: MSELoss
